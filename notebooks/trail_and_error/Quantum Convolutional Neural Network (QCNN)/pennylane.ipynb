{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 11:02:35,368 - INFO - Initializing the Quantum Convolutional Neural Network (QCNN)\n",
      "2024-12-24 11:02:35,369 - INFO - Initializing Data Loaders\n",
      "2024-12-24 11:02:35,369 - INFO - Loading dataset from directory: /Users/sahajrajmalla/Documents/nepali-quantum-mnist/data/numerals\n",
      "2024-12-24 11:02:35,400 - INFO - Loaded 2880 samples.\n",
      "2024-12-24 11:02:35,413 - INFO - Data Loaders initialized successfully: 2304 training samples, 576 testing samples\n",
      "2024-12-24 11:02:35,427 - INFO - Model architecture:\n",
      "QCNN(\n",
      "  (quantum_conv): QuantumConvolution()\n",
      "  (pool): QuantumPooling()\n",
      "  (quantum_fc): QuantumFullyConnected(\n",
      "    (fc): Linear(in_features=49, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "2024-12-24 11:02:35,428 - INFO - Starting training process\n",
      "2024-12-24 11:02:35,428 - INFO - Starting training for 20 epochs\n",
      "                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 460\u001b[0m\n\u001b[1;32m    457\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantum Convolutional Neural Network execution completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 448\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    446\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel architecture:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, model)\n\u001b[1;32m    447\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training process\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust epochs as needed\u001b[39;00m\n\u001b[1;32m    450\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting evaluation on test data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    451\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[1], line 360\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m    357\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    359\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (batch_size, 10)\u001b[39;00m\n\u001b[1;32m    361\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m    362\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 329\u001b[0m, in \u001b[0;36mQCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    327\u001b[0m patches \u001b[38;5;241m=\u001b[39m patches \u001b[38;5;241m/\u001b[39m patches\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Pass through quantum convolutional layer\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m conv_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantum_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, n_patches, n_qubits)\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Pass through pooling layer\u001b[39;00m\n\u001b[1;32m    331\u001b[0m pooled_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(conv_out)  \u001b[38;5;66;03m# (batch_size, n_patches, 1)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 217\u001b[0m, in \u001b[0;36mQuantumConvolution.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_patches):\n\u001b[1;32m    216\u001b[0m     patch \u001b[38;5;241m=\u001b[39m inputs[b, p]\n\u001b[0;32m--> 217\u001b[0m     q_out \u001b[38;5;241m=\u001b[39m \u001b[43mqnode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass entire params\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     q_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(q_out, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Ensure float32\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     patch_outputs\u001b[38;5;241m.\u001b[39mappend(q_out)\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/pennylane/workflow/qnode.py:1020\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mcapture\u001b[38;5;241m.\u001b[39menabled():\n\u001b[1;32m   1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mcapture\u001b[38;5;241m.\u001b[39mqnode_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/pennylane/workflow/qnode.py:1008\u001b[0m, in \u001b[0;36mQNode._impl_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_gradient_fn(shots\u001b[38;5;241m=\u001b[39moverride_shots, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape)\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride_shots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_shots\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_interface \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/pennylane/workflow/qnode.py:945\u001b[0m, in \u001b[0;36mQNode._execution_component\u001b[0;34m(self, args, kwargs, override_shots)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;66;03m# Calculate the classical jacobians if necessary\u001b[39;00m\n\u001b[1;32m    944\u001b[0m full_transform_program\u001b[38;5;241m.\u001b[39mset_classical_component(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m--> 945\u001b[0m \u001b[43m_prune_dynamic_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_transform_program\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_transform_program\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m execute_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmcm_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m mcm_config\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;66;03m# TODO: remove this once the cycle for the arguments have finished, i.e. 0.39.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/pennylane/workflow/qnode.py:1049\u001b[0m, in \u001b[0;36m_prune_dynamic_transform\u001b[0;34m(outer_transform, inner_transform)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_to_keep \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m dynamic_transform_found \u001b[38;5;241m=\u001b[39m \u001b[43minner_transform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_dynamic_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_to_keep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dynamic_transform_found:\n\u001b[1;32m   1051\u001b[0m     type_to_keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/pennylane/transforms/core/transform_program.py:375\u001b[0m, in \u001b[0;36mTransformProgram.prune_dynamic_transform\u001b[0;34m(self, type_to_keep)\u001b[0m\n\u001b[1;32m    373\u001b[0m     type_to_keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# keep this and do not keep the rest\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamic_one_shot\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmid_circuit_measurements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(t):\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_program\u001b[38;5;241m.\u001b[39mpop(i)\n\u001b[1;32m    377\u001b[0m i \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/nepali-quantum-mnist/venv/lib/python3.9/site-packages/pennylane/transforms/core/transform_dispatcher.py:385\u001b[0m, in \u001b[0;36mTransformContainer.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_transform \u001b[38;5;241m=\u001b[39m is_informative \u001b[38;5;129;01mor\u001b[39;00m final_transform\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_argnum \u001b[38;5;241m=\u001b[39m use_argnum\n\u001b[0;32m--> 385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# qcnn_classifier.py\n",
    "\n",
    "\"\"\"\n",
    "Quantum Convolutional Neural Network (QCNN) for Nepali Numerals Classification.\n",
    "\n",
    "This script implements a QCNN using PennyLane and PyTorch to classify Nepali numerals (0-9)\n",
    "based on 28x28 pixel grayscale images. The dataset should be organized with images in\n",
    "subdirectories per class.\n",
    "\n",
    "Directory Structure:\n",
    "your_project/\n",
    "├── qcnn_classifier.py\n",
    "└── data/\n",
    "    └── numerals/\n",
    "        ├── 0/\n",
    "        │   ├── image1.png\n",
    "        │   ├── image2.png\n",
    "        │   └── ...\n",
    "        ├── 1/\n",
    "        │   ├── image1.png\n",
    "        │   ├── image2.png\n",
    "        │   └── ...\n",
    "        └── ...\n",
    "        └── 9/\n",
    "            ├── image1.png\n",
    "            ├── image2.png\n",
    "            └── ...\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "# ==============================\n",
    "# Logging Setup\n",
    "# ==============================\n",
    "def setup_logging(log_file='training.log'):\n",
    "    \"\"\"\n",
    "    Sets up logging to output to both console and a file with a specific format.\n",
    "    Args:\n",
    "        log_file (str): Filename for the log file.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger('QCNNClassifier')\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Create handlers\n",
    "    c_handler = logging.StreamHandler()\n",
    "    f_handler = logging.FileHandler(log_file, mode='w')\n",
    "\n",
    "    c_handler.setLevel(logging.INFO)\n",
    "    f_handler.setLevel(logging.INFO)\n",
    "\n",
    "    # Create formatter and add it to handlers\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    c_handler.setFormatter(formatter)\n",
    "    f_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add handlers to the logger\n",
    "    if not logger.handlers:\n",
    "        logger.addHandler(c_handler)\n",
    "        logger.addHandler(f_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "# ==============================\n",
    "# Dataset Definition\n",
    "# ==============================\n",
    "class NepaliMNISTDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, patch_size=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images organized in subdirectories per class.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            patch_size (int): Size of each patch (patch_size x patch_size).\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        logger.info(f\"Loading dataset from directory: {root_dir}\")\n",
    "        for label in range(10):\n",
    "            label_dir = os.path.join(root_dir, str(label))\n",
    "            if not os.path.isdir(label_dir):\n",
    "                logger.warning(f\"Directory for label {label} does not exist: {label_dir}\")\n",
    "                continue\n",
    "            for img_file in os.listdir(label_dir):\n",
    "                img_path = os.path.join(label_dir, img_file)\n",
    "                if os.path.isfile(img_path) and img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                    self.data.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "        logger.info(f\"Loaded {len(self.data)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.data[idx]).convert('L')  # Convert to grayscale\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# ==============================\n",
    "# Data Transformations\n",
    "# ==============================\n",
    "# Define the transformation pipeline for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),  # Resize to 28x28 pixels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# ==============================\n",
    "# Data Loaders Initialization\n",
    "# ==============================\n",
    "ROOT_DIR = \"/Users/sahajrajmalla/Documents/nepali-quantum-mnist/data/numerals\"  # Update this path as needed\n",
    "\n",
    "def initialize_data_loaders(root_dir, batch_size=16, patch_size=4):\n",
    "    \"\"\"\n",
    "    Initializes and returns the training and testing data loaders.\n",
    "    Args:\n",
    "        root_dir (str): Root directory of the dataset.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        patch_size (int): Size of each image patch.\n",
    "    Returns:\n",
    "        Tuple[DataLoader, DataLoader]: Training and testing data loaders.\n",
    "    \"\"\"\n",
    "    logger.info(\"Initializing Data Loaders\")\n",
    "    dataset = NepaliMNISTDataset(root_dir=root_dir, transform=transform, patch_size=patch_size)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 to avoid multiprocessing issues in scripts or notebooks\n",
    "        pin_memory=True  # Optional: can speed up data transfer to GPU\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Set to 0 to avoid multiprocessing issues in scripts or notebooks\n",
    "        pin_memory=True  # Optional\n",
    "    )\n",
    "    logger.info(\"Data Loaders initialized successfully: %d training samples, %d testing samples\", train_size, test_size)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ==============================\n",
    "# QCNN Layers Definitions\n",
    "# ==============================\n",
    "def get_qcnn_layers(n_qubits=4, depth=1):\n",
    "    \"\"\"\n",
    "    Defines the QCNN layers using PennyLane templates.\n",
    "    Args:\n",
    "        n_qubits (int): Number of qubits per patch.\n",
    "        depth (int): Number of convolutional layers.\n",
    "    Returns:\n",
    "        list: List of QCNN layers, each layer is a list of tuples (gate_type, wires).\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for _ in range(depth):\n",
    "        conv_layer = []\n",
    "        for i in range(n_qubits):\n",
    "            conv_layer.append(('Rot', i))  # Parameterized gate: Rot\n",
    "        # Entangling layer\n",
    "        for i in range(n_qubits - 1):\n",
    "            conv_layer.append(('CNOT', [i, i + 1]))  # Non-parameterized gate: CNOT\n",
    "        layers.append(conv_layer)\n",
    "    return layers\n",
    "\n",
    "# ==============================\n",
    "# Quantum Convolutional Layer\n",
    "# ==============================\n",
    "class QuantumConvolution(nn.Module):\n",
    "    def __init__(self, n_qubits=4, depth=1):\n",
    "        super(QuantumConvolution, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.depth = depth\n",
    "        self.layers = get_qcnn_layers(n_qubits, depth)\n",
    "        # Initialize parameters: for each Rot gate, three angles (phi, theta, omega)\n",
    "        # The number of Rot gates per layer is n_qubits\n",
    "        # For CNOT gates, parameters are unused but need to maintain the structure\n",
    "        self.params = nn.Parameter(0.01 * torch.randn(len(self.layers), len(self.layers[0]), 3))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the quantum convolutional layer.\n",
    "        Args:\n",
    "            inputs (Tensor): Input tensor of shape (batch_size, n_patches, n_qubits)\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, n_patches, n_qubits)\n",
    "        \"\"\"\n",
    "        batch_size, n_patches, _ = inputs.shape  # inputs shape: (batch_size, n_patches, n_qubits)\n",
    "        outputs = []\n",
    "        for b in range(batch_size):\n",
    "            patch_outputs = []\n",
    "            for p in range(n_patches):\n",
    "                patch = inputs[b, p]\n",
    "                q_out = qnode(self.params, self.layers, patch)  # Pass entire params\n",
    "                q_out = torch.tensor(q_out, dtype=torch.float32, device=patch.device)  # Ensure float32\n",
    "                patch_outputs.append(q_out)\n",
    "            # Stack patch outputs: shape (n_patches, n_qubits)\n",
    "            outputs.append(torch.stack(patch_outputs))\n",
    "        # Stack all quantum outputs: shape (batch_size, n_patches, n_qubits)\n",
    "        return torch.stack(outputs)\n",
    "\n",
    "# ==============================\n",
    "# Quantum Node Definition\n",
    "# ==============================\n",
    "n_qubits_conv = 4  # Number of qubits per patch\n",
    "dev_conv = qml.device(\"default.qubit\", wires=n_qubits_conv)\n",
    "\n",
    "@qml.qnode(dev_conv, interface='torch', diff_method='parameter-shift')\n",
    "def qnode(params, layers, inputs):\n",
    "    \"\"\"\n",
    "    Defines a quantum circuit for convolutional layers.\n",
    "    Args:\n",
    "        params (Tensor): Parameters for rotation gates, shape (n_layers, n_gates_per_layer, 3).\n",
    "        layers (list): List of QCNN layers, each layer is a list of tuples (gate_type, wires).\n",
    "        inputs (Tensor): Input data encoded into the circuit, shape (n_qubits,).\n",
    "    Returns:\n",
    "        Tensor: Measurement results from all qubits.\n",
    "    \"\"\"\n",
    "    # Amplitude Encoding\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits_conv), normalize=True, pad_with=0.0)\n",
    "\n",
    "    # Apply QCNN layers\n",
    "    for layer, param_layer in zip(layers, params):\n",
    "        for gate_info, param in zip(layer, param_layer):\n",
    "            gate_type, wires = gate_info\n",
    "            if gate_type == 'Rot':\n",
    "                phi, theta, omega = param\n",
    "                qml.Rot(phi, theta, omega, wires=wires)\n",
    "            elif gate_type == 'CNOT':\n",
    "                qml.CNOT(wires=wires)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown gate type: {gate_type}\")\n",
    "\n",
    "    # Measurement: Expectation value of PauliZ on all qubits\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits_conv)]\n",
    "\n",
    "# ==============================\n",
    "# Quantum Pooling Layer\n",
    "# ==============================\n",
    "class QuantumPooling(nn.Module):\n",
    "    def __init__(self, n_qubits=4):\n",
    "        super(QuantumPooling, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the quantum pooling layer.\n",
    "        Args:\n",
    "            inputs (Tensor): Input tensor of shape (batch_size, n_patches, n_qubits)\n",
    "        Returns:\n",
    "            Tensor: Pooled tensor of shape (batch_size, n_patches, 1)\n",
    "        \"\"\"\n",
    "        # Simple pooling: average over qubits\n",
    "        return torch.mean(inputs, dim=2, keepdim=True)  # Shape: (batch_size, n_patches, 1)\n",
    "\n",
    "# ==============================\n",
    "# Quantum Fully Connected Layer\n",
    "# ==============================\n",
    "class QuantumFullyConnected(nn.Module):\n",
    "    def __init__(self, input_dim=49, output_dim=10):\n",
    "        super(QuantumFullyConnected, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)  # Mapping to 10 classes\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the quantum fully connected layer.\n",
    "        Args:\n",
    "            inputs (Tensor): Input tensor of shape (batch_size, n_patches, n_qubits)\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, 10)\n",
    "        \"\"\"\n",
    "        batch_size = inputs.shape[0]\n",
    "        x = inputs.view(batch_size, -1)  # Shape: (batch_size, input_dim)\n",
    "        out = self.fc(x)  # Shape: (batch_size, output_dim)\n",
    "        return out\n",
    "\n",
    "# ==============================\n",
    "# QCNN Model Definition\n",
    "# ==============================\n",
    "class QCNN(nn.Module):\n",
    "    def __init__(self, n_qubits=4, depth=1, patch_size=4):\n",
    "        super(QCNN, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.depth = depth\n",
    "        self.patch_size = patch_size\n",
    "        self.quantum_conv = QuantumConvolution(n_qubits=self.n_qubits, depth=self.depth)\n",
    "        self.pool = QuantumPooling(n_qubits=1)\n",
    "        self.quantum_fc = QuantumFullyConnected(input_dim=49, output_dim=10)  # Updated input_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the QCNN model.\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, 1, 28, 28)\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, 10)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        # Divide image into patches using unfold\n",
    "        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        # patches shape: (batch_size, channels, n_patches_h, n_patches_w, patch_size, patch_size)\n",
    "        patches = patches.contiguous().view(batch_size, -1, self.patch_size * self.patch_size)  # (batch_size, n_patches, 16)\n",
    "        # Normalize patches\n",
    "        patches = patches / patches.norm(dim=2, keepdim=True)\n",
    "        # Pass through quantum convolutional layer\n",
    "        conv_out = self.quantum_conv(patches)  # (batch_size, n_patches, n_qubits)\n",
    "        # Pass through pooling layer\n",
    "        pooled_out = self.pool(conv_out)  # (batch_size, n_patches, 1)\n",
    "        # Pass through fully connected layer\n",
    "        fc_out = self.quantum_fc(pooled_out)  # (batch_size, 10)\n",
    "        return fc_out\n",
    "\n",
    "# ==============================\n",
    "# Training Function\n",
    "# ==============================\n",
    "def train_model(model, train_loader, optimizer, loss_fn, epochs, device):\n",
    "    \"\"\"\n",
    "    Trains the model for a specified number of epochs.\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for training.\n",
    "        loss_fn (nn.Module): Loss function.\n",
    "        epochs (int): Number of training epochs.\n",
    "        device (torch.device): Device to run the training on.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting training for %d epochs\", epochs)\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}/{epochs}', leave=False)\n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)  # Shape: (batch_size, 1, 28, 28)\n",
    "            labels = labels.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)  # Shape: (batch_size, 10)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({'Loss': loss.item()})\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch}/{epochs} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# Evaluation Function\n",
    "# ==============================\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and returns the accuracy.\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        test_loader (DataLoader): DataLoader for testing data.\n",
    "        device (torch.device): Device to run the evaluation on.\n",
    "    Returns:\n",
    "        float: Accuracy percentage.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting evaluation\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Evaluating', leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    logger.info(f\"Evaluation completed with Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# ==============================\n",
    "# Visualization Function\n",
    "# ==============================\n",
    "def visualize_predictions(model, test_loader, device, num_images=6):\n",
    "    \"\"\"\n",
    "    Visualizes a few predictions from the model.\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        test_loader (DataLoader): DataLoader for testing data.\n",
    "        device (torch.device): Device to run the visualization on.\n",
    "        num_images (int): Number of images to visualize.\n",
    "    \"\"\"\n",
    "    logger.info(\"Visualizing predictions\")\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(images)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for idx, ax in enumerate(axes):\n",
    "        if idx >= len(images):\n",
    "            break\n",
    "        # Reshape the image to 28x28\n",
    "        image = images[idx].cpu().numpy().reshape(28, 28)\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_title(f\"True: {labels[idx].item()}\\nPred: {predictions[idx].item()}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    logger.info(\"Visualization complete\")\n",
    "\n",
    "# ==============================\n",
    "# Main Execution\n",
    "# ==============================\n",
    "def main():\n",
    "    logger.info(\"Initializing the Quantum Convolutional Neural Network (QCNN)\")\n",
    "    train_loader, test_loader = initialize_data_loaders(ROOT_DIR, batch_size=16, patch_size=4)\n",
    "\n",
    "    model = QCNN(n_qubits=4, depth=1, patch_size=4)  # You can increase depth for more complex models\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)  # Adjust learning rate as needed\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    logger.info(\"Model architecture:\\n%s\", model)\n",
    "    logger.info(\"Starting training process\")\n",
    "    train_model(model, train_loader, optimizer, loss_fn, epochs=20, device=device)  # Adjust epochs as needed\n",
    "\n",
    "    logger.info(\"Starting evaluation on test data\")\n",
    "    accuracy = evaluate_model(model, test_loader, device=device)\n",
    "    logger.info(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    logger.info(\"Starting visualization of predictions\")\n",
    "    visualize_predictions(model, test_loader, device=device)\n",
    "\n",
    "    logger.info(\"Quantum Convolutional Neural Network execution completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2024-12-24 10:06:13,298 - INFO - Initializing the Quantum Convolutional Neural Network (QCNN)\n",
    "# 2024-12-24 10:06:13,298 - INFO - Initializing Data Loaders\n",
    "# 2024-12-24 10:06:13,299 - INFO - Loading dataset from directory: /Users/sahajrajmalla/Documents/nepali-quantum-mnist/data/numerals\n",
    "# 2024-12-24 10:06:13,324 - INFO - Loaded 2880 samples.\n",
    "# 2024-12-24 10:06:13,335 - INFO - Data Loaders initialized successfully: 2304 training samples, 576 testing samples\n",
    "# 2024-12-24 10:06:13,350 - INFO - Model architecture:\n",
    "# QCNN(\n",
    "#   (quantum_conv): QuantumConvolution()\n",
    "#   (pool): QuantumPooling()\n",
    "#   (quantum_fc): QuantumFullyConnected(\n",
    "#     (fc): Linear(in_features=49, out_features=10, bias=True)\n",
    "#   )\n",
    "# )\n",
    "# 2024-12-24 10:06:13,351 - INFO - Starting training process\n",
    "# 2024-12-24 10:06:13,352 - INFO - Starting training for 20 epochs\n",
    "# 2024-12-24 10:08:55,178 - INFO - Epoch 1/20 - Average Loss: 2.2502      \n",
    "# 2024-12-24 10:11:29,459 - INFO - Epoch 2/20 - Average Loss: 2.1320      \n",
    "# 2024-12-24 10:14:04,471 - INFO - Epoch 3/20 - Average Loss: 2.0383      \n",
    "# 2024-12-24 10:16:33,356 - INFO - Epoch 4/20 - Average Loss: 1.9627      \n",
    "# 2024-12-24 10:19:02,626 - INFO - Epoch 5/20 - Average Loss: 1.9000      \n",
    "# 2024-12-24 10:21:35,039 - INFO - Epoch 6/20 - Average Loss: 1.8490      \n",
    "# 2024-12-24 10:24:22,218 - INFO - Epoch 7/20 - Average Loss: 1.8062      \n",
    "# 2024-12-24 10:27:35,139 - INFO - Epoch 8/20 - Average Loss: 1.7706      \n",
    "# 2024-12-24 10:30:27,833 - INFO - Epoch 9/20 - Average Loss: 1.7396      \n",
    "# 2024-12-24 10:32:55,537 - INFO - Epoch 10/20 - Average Loss: 1.7135      \n",
    "# ...\n",
    "# 2024-12-24 11:00:51,124 - INFO - Evaluation completed with Accuracy: 45.31%\n",
    "# 2024-12-24 11:00:51,126 - INFO - Test Accuracy: 45.31%\n",
    "# 2024-12-24 11:00:51,126 - INFO - Starting visualization of predictions\n",
    "# 2024-12-24 11:00:51,127 - INFO - Visualizing predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
