{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ecfe52b",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "- Download the **Devanagari Character Dataset (Large)** from Kaggle:  \n",
    "  [https://www.kaggle.com/datasets/ashokpant/devanagari-character-dataset-large](https://www.kaggle.com/datasets/ashokpant/devanagari-character-dataset-large)\n",
    "\n",
    "- After downloading the `.rar` file to your local machine, upload it to the Colab or Jupyter notebook environment at `/content/` using the file upload interface.\n",
    "\n",
    "- To extract the `.rar` file, run the following commands in a notebook code cell:\n",
    "\n",
    "  ```bash\n",
    "  !unrar x dhcd.rar\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23d12c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf test/ train/\n",
    "!apt-get install -y unrar\n",
    "!unrar x dhcd.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d40aa9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.amp import autocast, GradScaler\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Setup logging\n",
    "def setup_logging():\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Remove any existing handlers\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    file_handler = logging.FileHandler('training.log', mode='w')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "\n",
    "    stream_handler = logging.StreamHandler(sys.stdout)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    stream_handler.setLevel(logging.INFO)\n",
    "\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(stream_handler)\n",
    "\n",
    "    # Patch FileHandler emit to flush after each log entry\n",
    "    old_emit = file_handler.emit\n",
    "    def emit_and_flush(record):\n",
    "        old_emit(record)\n",
    "        file_handler.flush()\n",
    "    file_handler.emit = emit_and_flush\n",
    "\n",
    "    return logger\n",
    "\n",
    "# Custom transform for data augmentation\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=0.05):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "\n",
    "# Residual block with configurable dropout\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout_rate=0.05):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Homogeneous Filter Capsule layer\n",
    "class HFCLayer(nn.Module):\n",
    "    def __init__(self, num_classes, D_b):\n",
    "        super(HFCLayer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.V = nn.Parameter(torch.randn(num_classes, D_b))\n",
    "        self.bn = nn.BatchNorm1d(num_classes * D_b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        U_b = x.sum(dim=1)\n",
    "        U_b_exp = U_b.unsqueeze(1)\n",
    "        V_exp = self.V.unsqueeze(0)\n",
    "        T_b = U_b_exp * V_exp\n",
    "        batch_size = T_b.size(0)\n",
    "        T_b_flat = T_b.view(batch_size, -1)\n",
    "        T_b_bn = self.bn(T_b_flat)\n",
    "        T_b_bn = T_b_bn.view(batch_size, self.num_classes, -1)\n",
    "        T_b_relu = F.relu(T_b_bn)\n",
    "        logits = T_b_relu.sum(dim=2)\n",
    "        return logits\n",
    "\n",
    "# Merging layer for combining branch outputs\n",
    "class MergingLayer(nn.Module):\n",
    "    def __init__(self, num_branches=3):\n",
    "        super(MergingLayer, self).__init__()\n",
    "        self.w = nn.Parameter(torch.ones(num_branches) / num_branches)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weights = F.softmax(self.w, dim=0)\n",
    "        return sum(w * logit for w, logit in zip(weights, inputs))\n",
    "\n",
    "# Enhanced BMCNN base with increased capacity\n",
    "class BMCNNBase(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.05):\n",
    "        super(BMCNNBase, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            ResidualBlock(1, 128, stride=1, dropout_rate=dropout_rate),\n",
    "            ResidualBlock(128, 128, stride=1, dropout_rate=dropout_rate),\n",
    "            ResidualBlock(128, 128, stride=1, dropout_rate=dropout_rate)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            ResidualBlock(128, 256, stride=1, dropout_rate=dropout_rate),\n",
    "            ResidualBlock(256, 256, stride=1, dropout_rate=dropout_rate),\n",
    "            ResidualBlock(256, 256, stride=1, dropout_rate=dropout_rate)\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            ResidualBlock(256, 512, stride=1, dropout_rate=dropout_rate),\n",
    "            ResidualBlock(512, 512, stride=1, dropout_rate=dropout_rate),\n",
    "            ResidualBlock(512, 512, stride=1, dropout_rate=dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_block1(x)\n",
    "        x = self.pool1(x1)\n",
    "        x2 = self.conv_block2(x)\n",
    "        x = self.pool2(x2)\n",
    "        x3 = self.conv_block3(x)\n",
    "        return x1, x2, x3\n",
    "\n",
    "# Enhanced BMCNN with HFCs\n",
    "class EnhancedBMCNNwHFCs(BMCNNBase):\n",
    "    def __init__(self, num_classes=46, dropout_rate=0.05):\n",
    "        super(EnhancedBMCNNwHFCs, self).__init__(dropout_rate)\n",
    "        self.hfc1 = HFCLayer(num_classes, D_b=32*32)\n",
    "        self.hfc2 = HFCLayer(num_classes, D_b=16*16)\n",
    "        self.hfc3 = HFCLayer(num_classes, D_b=8*8)\n",
    "        self.merging = MergingLayer(num_branches=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3 = super().forward(x)\n",
    "        x1_reshaped = x1.view(x1.size(0), x1.size(1), -1)\n",
    "        logit1 = self.hfc1(x1_reshaped)\n",
    "        x2_reshaped = x2.view(x2.size(0), x2.size(1), -1)\n",
    "        logit2 = self.hfc2(x2_reshaped)\n",
    "        x3_reshaped = x3.view(x3.size(0), x3.size(1), -1)\n",
    "        logit3 = self.hfc3(x3_reshaped)\n",
    "        logits = self.merging((logit1, logit2, logit3))\n",
    "        return logits\n",
    "\n",
    "# Label smoothing loss\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=46, smoothing=0.05):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# Dataset for Nepali handwritten characters\n",
    "class NepaliMNISTDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, logger=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.logger = logger\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        if not os.path.exists(data_dir):\n",
    "            raise FileNotFoundError(f\"Dataset directory {data_dir} does not exist\")\n",
    "        for class_id in range(46):\n",
    "            class_dir = os.path.join(data_dir, str(class_id))\n",
    "            if not os.path.isdir(class_dir):\n",
    "                if self.logger:\n",
    "                    self.logger.warning(f\"Class directory {class_dir} not found\")\n",
    "                continue\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                if not os.path.isfile(img_path):\n",
    "                    if self.logger:\n",
    "                        self.logger.warning(f\"Image {img_path} is not a file\")\n",
    "                    continue\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(class_id)\n",
    "        if self.logger:\n",
    "            self.logger.info(f\"Loaded {len(self.image_paths)} images from {data_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('L')\n",
    "        except Exception as e:\n",
    "            if self.logger:\n",
    "                self.logger.error(f\"Failed to load image {img_path}: {str(e)}\")\n",
    "            raise\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Training function with logging\n",
    "def train_models(config, train_dataset, val_dataset, device, logger):\n",
    "    num_models = config.get('num_models', 1)\n",
    "    seeds = config.get('seeds', [42])\n",
    "    if len(seeds) != num_models:\n",
    "        seeds = [42 + i for i in range(num_models)]\n",
    "\n",
    "    # Initialize epoch log\n",
    "    epoch_log_file = 'epoch_logs.csv'\n",
    "    if not os.path.exists(epoch_log_file):\n",
    "        with open(epoch_log_file, 'w') as f:\n",
    "            f.write(\"config_id,seed,epoch,train_loss,train_acc,val_loss,val_acc,epoch_time\\n\")\n",
    "\n",
    "    # Results dictionary to store metrics\n",
    "    config_results = {\n",
    "        'config': config,\n",
    "        'models': []\n",
    "    }\n",
    "\n",
    "    for model_idx in range(num_models):\n",
    "        logger.info(f\"Training model {model_idx+1}/{num_models} with seed {seeds[model_idx]}\")\n",
    "        # Set seeds for reproducibility\n",
    "        torch.manual_seed(seeds[model_idx])\n",
    "        torch.cuda.manual_seed(seeds[model_idx])\n",
    "        np.random.seed(seeds[model_idx])\n",
    "        random.seed(seeds[model_idx])\n",
    "\n",
    "        model = EnhancedBMCNNwHFCs(num_classes=46, dropout_rate=config['dropout']).to(device)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['lr'],\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        criterion = LabelSmoothingLoss(classes=46, smoothing=config.get('label_smoothing', 0.05))\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=4,\n",
    "            drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=4,\n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_acc = 0.0\n",
    "        patience_counter = 0\n",
    "        epochs = config.get('epochs', 2)\n",
    "        patience = config.get('patience', 20)\n",
    "        best_model_state = None\n",
    "\n",
    "        # Model-specific metrics\n",
    "        model_metrics = {\n",
    "            'seed': seeds[model_idx],\n",
    "            'epochs': []\n",
    "        }\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                with autocast('cuda'):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "            train_loss = running_loss / len(train_dataset)\n",
    "            train_acc = correct / total\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images = images.to(device, non_blocking=True)\n",
    "                    labels = labels.to(device, non_blocking=True)\n",
    "                    with autocast('cuda'):\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "            val_loss /= len(val_dataset)\n",
    "            val_acc = correct / total\n",
    "\n",
    "            end_time = time.time()\n",
    "            epoch_time = end_time - start_time\n",
    "\n",
    "            # Log metrics\n",
    "            epoch_metrics = {\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'epoch_time': epoch_time\n",
    "            }\n",
    "            model_metrics['epochs'].append(epoch_metrics)\n",
    "\n",
    "            logger.info(\n",
    "                f\"Config: {config}, Model {model_idx+1}, Epoch {epoch+1}/{epochs} - \"\n",
    "                f\"Train Loss: {train_loss:.4f} - Train Acc: {train_acc:.4f} - \"\n",
    "                f\"Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f} - \"\n",
    "                f\"Epoch Time: {epoch_time:.2f}s\"\n",
    "            )\n",
    "            config_id = f\"lr{config['lr']}_bs{config['batch_size']}_dr{config['dropout']}_ls{config['label_smoothing']}\"\n",
    "            with open('epoch_logs.csv', 'a') as f:\n",
    "                f.write(f\"{config_id},{seeds[model_idx]},{epoch+1},{train_loss:.4f},{train_acc:.4f},{val_loss:.4f},{val_acc:.4f},{epoch_time:.2f}\\n\")\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model_state = model.state_dict()\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    logger.info(f\"Model {model_idx+1} Early stopping triggered\")\n",
    "                    break\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        config_results['models'].append(model_metrics)\n",
    "\n",
    "    return config_results, best_val_acc, best_model_state\n",
    "\n",
    "# Evaluation function with detailed metrics\n",
    "def evaluate_model(model, test_loader, device, criterion, logger, num_classes=46):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = correct / total\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None, labels=range(num_classes), zero_division=0\n",
    "    )\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "\n",
    "    logger.info(f\"Test Loss: {test_loss:.4f} - Test Acc: {test_acc:.4f}\")\n",
    "    return test_loss, test_acc, precision, recall, f1, conf_matrix\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger = setup_logging()\n",
    "    logger.info(f\"Starting experiment on {device}\")\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}, Device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # Data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "        AddGaussianNoise(mean=0., std=0.03)\n",
    "    ])\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = NepaliMNISTDataset(\n",
    "        data_dir='/content/train',\n",
    "        transform=train_transform,\n",
    "        logger=logger\n",
    "    )\n",
    "    test_dataset = NepaliMNISTDataset(\n",
    "        data_dir='/content/test',\n",
    "        transform=val_test_transform,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        list(range(len(train_dataset))),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=train_dataset.labels\n",
    "    )\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(train_dataset, val_indices)\n",
    "\n",
    "    # Fixed parameters\n",
    "    fixed_params = {\n",
    "        'model_type': 'hfc',\n",
    "        'num_models': 1,\n",
    "        'seeds': [42],\n",
    "        'epochs': 2,\n",
    "        'patience': 20\n",
    "    }\n",
    "\n",
    "    # Hyperparameters\n",
    "    hyperparams = [\n",
    "        {'lr': lr, 'batch_size': bs, 'dropout': dr, 'label_smoothing': ls}\n",
    "        for lr in [1e-3, 5e-4]\n",
    "        for bs in [64, 128]\n",
    "        for dr in [0.0, 0.1]\n",
    "        for ls in [0.0, 0.1]\n",
    "    ]\n",
    "\n",
    "    hyperparam_results = []\n",
    "    best_val_acc = 0.0\n",
    "    best_config = None\n",
    "    best_model_state = None\n",
    "\n",
    "    for hp in hyperparams:\n",
    "        config = fixed_params.copy()\n",
    "        config.update(hp)\n",
    "        config_results, val_acc, model_state = train_models(config, train_subset, val_subset, device, logger)\n",
    "\n",
    "        # Aggregate best metrics\n",
    "        best_train_loss = float('inf')\n",
    "        best_train_acc = 0.0\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_acc_config = 0.0\n",
    "        for model in config_results['models']:\n",
    "            for epoch_data in model['epochs']:\n",
    "                best_train_loss = min(best_train_loss, epoch_data['train_loss'])\n",
    "                best_train_acc = max(best_train_acc, epoch_data['train_acc'])\n",
    "                best_val_loss = min(best_val_loss, epoch_data['val_loss'])\n",
    "                best_val_acc_config = max(best_val_acc_config, epoch_data['val_acc'])\n",
    "\n",
    "        result_entry = {\n",
    "            'lr': hp['lr'],\n",
    "            'batch_size': hp['batch_size'],\n",
    "            'dropout': hp['dropout'],\n",
    "            'label_smoothing': hp['label_smoothing'],\n",
    "            'best_train_loss': best_train_loss,\n",
    "            'best_train_acc': best_train_acc,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_acc': best_val_acc_config\n",
    "        }\n",
    "        hyperparam_results.append(result_entry)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_config = config.copy()\n",
    "            best_model_state = model_state\n",
    "\n",
    "        pd.DataFrame(hyperparam_results).to_csv('hyperparam_results.csv', index=False)\n",
    "        logger.info(f\"Saved hyperparam results for config: {config}\")\n",
    "        logger.info(f\"Best metrics for config {config}: {result_entry}\")\n",
    "\n",
    "    # Save best model and evaluate\n",
    "    if best_config is None or best_model_state is None:\n",
    "        raise ValueError(\"No valid best config or model state found\")\n",
    "    logger.info(f\"Saving best model with config: {best_config}\")\n",
    "    model = EnhancedBMCNNwHFCs(num_classes=46, dropout_rate=best_config['dropout']).to(device)\n",
    "    model.load_state_dict(best_model_state)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': best_config,\n",
    "        'val_acc': best_val_acc\n",
    "    }, 'best_model.pth')\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_config['batch_size'], shuffle=False, num_workers=4)\n",
    "    criterion = LabelSmoothingLoss(classes=46, smoothing=best_config['label_smoothing'])\n",
    "    test_loss, test_acc, precision, recall, f1, conf_matrix = evaluate_model(\n",
    "        model, test_loader, device, criterion, logger\n",
    "    )\n",
    "    metrics_summary = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'precision': precision.tolist(),\n",
    "        'recall': recall.tolist(),\n",
    "        'f1_score': f1.tolist(),\n",
    "        'conf_matrix': conf_matrix.tolist()\n",
    "    }\n",
    "    with open('test_metrics.csv', 'w') as f:\n",
    "        f.write(\"metric,value\\n\")\n",
    "        f.write(f\"test_loss,{test_loss:.4f}\\n\")\n",
    "        f.write(f\"test_acc,{test_acc:.4f}\\n\")\n",
    "        for i in range(46):\n",
    "            f.write(f\"precision_class_{i},{precision[i]:.4f}\\n\")\n",
    "            f.write(f\"recall_class_{i},{recall[i]:.4f}\\n\")\n",
    "            f.write(f\"f1_class_{i},{f1[i]:.4f}\\n\")\n",
    "    logger.info(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
